{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from mqbench.prepare_by_platform import prepare_by_platform, BackendType\n",
    "from mqbench.utils.state import enable_calibration, enable_quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Quantize model Scheme: BackendType.Tensorrt Mode: Eval\n",
      "[MQBENCH] INFO: Weight Qconfig:\n",
      "    FakeQuantize: LearnableFakeQuantize Params: {}\n",
      "    Oberver:      MinMaxObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: True / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Activation Qconfig:\n",
      "    FakeQuantize: LearnableFakeQuantize Params: {}\n",
      "    Oberver:      EMAMinMaxObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Replace module to qat module.\n",
      "[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant conv1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant maxpool_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_conv1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_conv2_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_conv1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_conv2_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_conv1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_conv2_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_downsample_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_conv1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_conv2_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_conv1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_conv2_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_downsample_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_conv1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_conv2_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_conv1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_conv2_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_downsample_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_1_conv1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_1_conv2_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant flatten_post_act_fake_quantizer\n"
     ]
    }
   ],
   "source": [
    "model_to_quantize = torchvision.models.__dict__['resnet18'](pretrained=True)\n",
    "model_to_quantize.cpu().eval()\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device='cpu')\n",
    "model_prepared = prepare_by_platform(model_to_quantize, BackendType.Tensorrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Enable observer and Disable quantize.\n",
      "[MQBENCH] INFO: Disable observer and Enable quantize.\n"
     ]
    }
   ],
   "source": [
    "enable_calibration(model_prepared)\n",
    "model_prepared(dummy_input)\n",
    "enable_quantization(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (conv1): ConvReLU2d(\n",
      "    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)\n",
      "    (weight_fake_quant): LearnableFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([64])], zero_point=List\n",
      "      (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "    )\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Module(\n",
      "    (0): Module(\n",
      "      (conv1): ConvReLU2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([64])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): Conv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([64])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): ConvReLU2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([64])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): Conv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([64])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Module(\n",
      "    (0): Module(\n",
      "      (conv1): ConvReLU2d(\n",
      "        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([128])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): Conv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([128])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (downsample): Module(\n",
      "        (0): Conv2d(\n",
      "          64, 128, kernel_size=(1, 1), stride=(2, 2)\n",
      "          (weight_fake_quant): LearnableFakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([128])], zero_point=List\n",
      "            (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): ConvReLU2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([128])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): Conv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([128])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Module(\n",
      "    (0): Module(\n",
      "      (conv1): ConvReLU2d(\n",
      "        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([256])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([256])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (downsample): Module(\n",
      "        (0): Conv2d(\n",
      "          128, 256, kernel_size=(1, 1), stride=(2, 2)\n",
      "          (weight_fake_quant): LearnableFakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([256])], zero_point=List\n",
      "            (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): ConvReLU2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([256])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([256])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Module(\n",
      "    (0): Module(\n",
      "      (conv1): ConvReLU2d(\n",
      "        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([512])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([512])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (downsample): Module(\n",
      "        (0): Conv2d(\n",
      "          256, 512, kernel_size=(1, 1), stride=(2, 2)\n",
      "          (weight_fake_quant): LearnableFakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([512])], zero_point=List\n",
      "            (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): ConvReLU2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([512])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): LearnableFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([512])], zero_point=List\n",
      "          (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "        )\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(\n",
      "    in_features=512, out_features=1000, bias=True\n",
      "    (weight_fake_quant): LearnableFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=List[torch.Size([1000])], zero_point=List\n",
      "      (activation_post_process): MinMaxObserver(min_val=List, max_val=List ch_axis=0 pot=False)\n",
      "    )\n",
      "  )\n",
      "  (x_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0369], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-4.700902462005615, max_val=4.363773822784424 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (conv1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0309], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=3.936420202255249 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (maxpool_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0309], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=3.936420202255249 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer1_0_conv1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0134], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=1.7048895359039307 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer1_0_conv2_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0197], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-2.0266542434692383, max_val=2.5137557983398438 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer1_0_relu_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0333], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=4.247805118560791 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer1_1_conv1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0157], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=1.9961326122283936 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer1_1_conv2_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0314], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-4.000840187072754, max_val=2.8356070518493652 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer1_1_relu_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0364], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=4.638629913330078 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer2_0_conv1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0185], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=2.35892915725708 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer2_0_conv2_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0181], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-1.9340059757232666, max_val=2.3031678199768066 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer2_0_downsample_0_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0249], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-3.170923948287964, max_val=2.656198740005493 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer2_0_relu_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0248], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=3.1605379581451416 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer2_1_conv1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0111], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=1.4173330068588257 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer2_1_conv2_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0182], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-2.3215792179107666, max_val=1.4043431282043457 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer2_1_relu_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0242], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=3.090801239013672 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer3_0_conv1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0095], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=1.2098462581634521 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer3_0_conv2_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0147], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-1.5378979444503784, max_val=1.8755807876586914 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer3_0_downsample_0_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0099], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-1.2682887315750122, max_val=0.506453275680542 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer3_0_relu_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0115], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=1.4603568315505981 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer3_1_conv1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0063], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=0.8050324320793152 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer3_1_conv2_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0161], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-2.059037685394287, max_val=1.0697687864303589 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer3_1_relu_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0113], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=1.4400241374969482 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer4_0_conv1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0048], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=0.6082836389541626 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer4_0_conv2_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0097], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-1.1752912998199463, max_val=1.235743522644043 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer4_0_downsample_0_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0108], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-1.37368905544281, max_val=1.0482028722763062 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer4_0_relu_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0100], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=1.2790316343307495 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer4_1_conv1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0049], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=0.6217929124832153 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer4_1_conv2_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0751], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=-3.3090553283691406, max_val=9.569875717163086 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (layer4_1_relu_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0793], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=10.115379333496094 ch_axis=-1 pot=False)\n",
      "  )\n",
      "  (flatten_post_act_fake_quantizer): LearnableFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
      "    tensor([0.0329], requires_grad=True), zero_point=Parameter containing:\n",
      "    tensor([0.], requires_grad=True)\n",
      "    (activation_post_process): EMAMinMaxObserver(min_val=0.0, max_val=4.191432476043701 ch_axis=-1 pot=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    x_post_act_fake_quantizer = self.x_post_act_fake_quantizer(x);  x = None\n",
      "    conv1 = self.conv1(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None\n",
      "    conv1_post_act_fake_quantizer = self.conv1_post_act_fake_quantizer(conv1);  conv1 = None\n",
      "    maxpool = self.maxpool(conv1_post_act_fake_quantizer);  conv1_post_act_fake_quantizer = None\n",
      "    maxpool_post_act_fake_quantizer = self.maxpool_post_act_fake_quantizer(maxpool);  maxpool = None\n",
      "    layer1_0_conv1 = getattr(self.layer1, \"0\").conv1(maxpool_post_act_fake_quantizer)\n",
      "    layer1_0_conv1_post_act_fake_quantizer = self.layer1_0_conv1_post_act_fake_quantizer(layer1_0_conv1);  layer1_0_conv1 = None\n",
      "    layer1_0_conv2 = getattr(self.layer1, \"0\").conv2(layer1_0_conv1_post_act_fake_quantizer);  layer1_0_conv1_post_act_fake_quantizer = None\n",
      "    layer1_0_conv2_post_act_fake_quantizer = self.layer1_0_conv2_post_act_fake_quantizer(layer1_0_conv2);  layer1_0_conv2 = None\n",
      "    add = layer1_0_conv2_post_act_fake_quantizer + maxpool_post_act_fake_quantizer;  layer1_0_conv2_post_act_fake_quantizer = maxpool_post_act_fake_quantizer = None\n",
      "    layer1_0_relu_1 = getattr(self.layer1, \"0\").relu(add);  add = None\n",
      "    layer1_0_relu_1_post_act_fake_quantizer = self.layer1_0_relu_1_post_act_fake_quantizer(layer1_0_relu_1);  layer1_0_relu_1 = None\n",
      "    layer1_1_conv1 = getattr(self.layer1, \"1\").conv1(layer1_0_relu_1_post_act_fake_quantizer)\n",
      "    layer1_1_conv1_post_act_fake_quantizer = self.layer1_1_conv1_post_act_fake_quantizer(layer1_1_conv1);  layer1_1_conv1 = None\n",
      "    layer1_1_conv2 = getattr(self.layer1, \"1\").conv2(layer1_1_conv1_post_act_fake_quantizer);  layer1_1_conv1_post_act_fake_quantizer = None\n",
      "    layer1_1_conv2_post_act_fake_quantizer = self.layer1_1_conv2_post_act_fake_quantizer(layer1_1_conv2);  layer1_1_conv2 = None\n",
      "    add_1 = layer1_1_conv2_post_act_fake_quantizer + layer1_0_relu_1_post_act_fake_quantizer;  layer1_1_conv2_post_act_fake_quantizer = layer1_0_relu_1_post_act_fake_quantizer = None\n",
      "    layer1_1_relu_1 = getattr(self.layer1, \"1\").relu(add_1);  add_1 = None\n",
      "    layer1_1_relu_1_post_act_fake_quantizer = self.layer1_1_relu_1_post_act_fake_quantizer(layer1_1_relu_1);  layer1_1_relu_1 = None\n",
      "    layer2_0_conv1 = getattr(self.layer2, \"0\").conv1(layer1_1_relu_1_post_act_fake_quantizer)\n",
      "    layer2_0_conv1_post_act_fake_quantizer = self.layer2_0_conv1_post_act_fake_quantizer(layer2_0_conv1);  layer2_0_conv1 = None\n",
      "    layer2_0_conv2 = getattr(self.layer2, \"0\").conv2(layer2_0_conv1_post_act_fake_quantizer);  layer2_0_conv1_post_act_fake_quantizer = None\n",
      "    layer2_0_conv2_post_act_fake_quantizer = self.layer2_0_conv2_post_act_fake_quantizer(layer2_0_conv2);  layer2_0_conv2 = None\n",
      "    layer2_0_downsample_0 = getattr(getattr(self.layer2, \"0\").downsample, \"0\")(layer1_1_relu_1_post_act_fake_quantizer);  layer1_1_relu_1_post_act_fake_quantizer = None\n",
      "    layer2_0_downsample_0_post_act_fake_quantizer = self.layer2_0_downsample_0_post_act_fake_quantizer(layer2_0_downsample_0);  layer2_0_downsample_0 = None\n",
      "    add_2 = layer2_0_conv2_post_act_fake_quantizer + layer2_0_downsample_0_post_act_fake_quantizer;  layer2_0_conv2_post_act_fake_quantizer = layer2_0_downsample_0_post_act_fake_quantizer = None\n",
      "    layer2_0_relu_1 = getattr(self.layer2, \"0\").relu(add_2);  add_2 = None\n",
      "    layer2_0_relu_1_post_act_fake_quantizer = self.layer2_0_relu_1_post_act_fake_quantizer(layer2_0_relu_1);  layer2_0_relu_1 = None\n",
      "    layer2_1_conv1 = getattr(self.layer2, \"1\").conv1(layer2_0_relu_1_post_act_fake_quantizer)\n",
      "    layer2_1_conv1_post_act_fake_quantizer = self.layer2_1_conv1_post_act_fake_quantizer(layer2_1_conv1);  layer2_1_conv1 = None\n",
      "    layer2_1_conv2 = getattr(self.layer2, \"1\").conv2(layer2_1_conv1_post_act_fake_quantizer);  layer2_1_conv1_post_act_fake_quantizer = None\n",
      "    layer2_1_conv2_post_act_fake_quantizer = self.layer2_1_conv2_post_act_fake_quantizer(layer2_1_conv2);  layer2_1_conv2 = None\n",
      "    add_3 = layer2_1_conv2_post_act_fake_quantizer + layer2_0_relu_1_post_act_fake_quantizer;  layer2_1_conv2_post_act_fake_quantizer = layer2_0_relu_1_post_act_fake_quantizer = None\n",
      "    layer2_1_relu_1 = getattr(self.layer2, \"1\").relu(add_3);  add_3 = None\n",
      "    layer2_1_relu_1_post_act_fake_quantizer = self.layer2_1_relu_1_post_act_fake_quantizer(layer2_1_relu_1);  layer2_1_relu_1 = None\n",
      "    layer3_0_conv1 = getattr(self.layer3, \"0\").conv1(layer2_1_relu_1_post_act_fake_quantizer)\n",
      "    layer3_0_conv1_post_act_fake_quantizer = self.layer3_0_conv1_post_act_fake_quantizer(layer3_0_conv1);  layer3_0_conv1 = None\n",
      "    layer3_0_conv2 = getattr(self.layer3, \"0\").conv2(layer3_0_conv1_post_act_fake_quantizer);  layer3_0_conv1_post_act_fake_quantizer = None\n",
      "    layer3_0_conv2_post_act_fake_quantizer = self.layer3_0_conv2_post_act_fake_quantizer(layer3_0_conv2);  layer3_0_conv2 = None\n",
      "    layer3_0_downsample_0 = getattr(getattr(self.layer3, \"0\").downsample, \"0\")(layer2_1_relu_1_post_act_fake_quantizer);  layer2_1_relu_1_post_act_fake_quantizer = None\n",
      "    layer3_0_downsample_0_post_act_fake_quantizer = self.layer3_0_downsample_0_post_act_fake_quantizer(layer3_0_downsample_0);  layer3_0_downsample_0 = None\n",
      "    add_4 = layer3_0_conv2_post_act_fake_quantizer + layer3_0_downsample_0_post_act_fake_quantizer;  layer3_0_conv2_post_act_fake_quantizer = layer3_0_downsample_0_post_act_fake_quantizer = None\n",
      "    layer3_0_relu_1 = getattr(self.layer3, \"0\").relu(add_4);  add_4 = None\n",
      "    layer3_0_relu_1_post_act_fake_quantizer = self.layer3_0_relu_1_post_act_fake_quantizer(layer3_0_relu_1);  layer3_0_relu_1 = None\n",
      "    layer3_1_conv1 = getattr(self.layer3, \"1\").conv1(layer3_0_relu_1_post_act_fake_quantizer)\n",
      "    layer3_1_conv1_post_act_fake_quantizer = self.layer3_1_conv1_post_act_fake_quantizer(layer3_1_conv1);  layer3_1_conv1 = None\n",
      "    layer3_1_conv2 = getattr(self.layer3, \"1\").conv2(layer3_1_conv1_post_act_fake_quantizer);  layer3_1_conv1_post_act_fake_quantizer = None\n",
      "    layer3_1_conv2_post_act_fake_quantizer = self.layer3_1_conv2_post_act_fake_quantizer(layer3_1_conv2);  layer3_1_conv2 = None\n",
      "    add_5 = layer3_1_conv2_post_act_fake_quantizer + layer3_0_relu_1_post_act_fake_quantizer;  layer3_1_conv2_post_act_fake_quantizer = layer3_0_relu_1_post_act_fake_quantizer = None\n",
      "    layer3_1_relu_1 = getattr(self.layer3, \"1\").relu(add_5);  add_5 = None\n",
      "    layer3_1_relu_1_post_act_fake_quantizer = self.layer3_1_relu_1_post_act_fake_quantizer(layer3_1_relu_1);  layer3_1_relu_1 = None\n",
      "    layer4_0_conv1 = getattr(self.layer4, \"0\").conv1(layer3_1_relu_1_post_act_fake_quantizer)\n",
      "    layer4_0_conv1_post_act_fake_quantizer = self.layer4_0_conv1_post_act_fake_quantizer(layer4_0_conv1);  layer4_0_conv1 = None\n",
      "    layer4_0_conv2 = getattr(self.layer4, \"0\").conv2(layer4_0_conv1_post_act_fake_quantizer);  layer4_0_conv1_post_act_fake_quantizer = None\n",
      "    layer4_0_conv2_post_act_fake_quantizer = self.layer4_0_conv2_post_act_fake_quantizer(layer4_0_conv2);  layer4_0_conv2 = None\n",
      "    layer4_0_downsample_0 = getattr(getattr(self.layer4, \"0\").downsample, \"0\")(layer3_1_relu_1_post_act_fake_quantizer);  layer3_1_relu_1_post_act_fake_quantizer = None\n",
      "    layer4_0_downsample_0_post_act_fake_quantizer = self.layer4_0_downsample_0_post_act_fake_quantizer(layer4_0_downsample_0);  layer4_0_downsample_0 = None\n",
      "    add_6 = layer4_0_conv2_post_act_fake_quantizer + layer4_0_downsample_0_post_act_fake_quantizer;  layer4_0_conv2_post_act_fake_quantizer = layer4_0_downsample_0_post_act_fake_quantizer = None\n",
      "    layer4_0_relu_1 = getattr(self.layer4, \"0\").relu(add_6);  add_6 = None\n",
      "    layer4_0_relu_1_post_act_fake_quantizer = self.layer4_0_relu_1_post_act_fake_quantizer(layer4_0_relu_1);  layer4_0_relu_1 = None\n",
      "    layer4_1_conv1 = getattr(self.layer4, \"1\").conv1(layer4_0_relu_1_post_act_fake_quantizer)\n",
      "    layer4_1_conv1_post_act_fake_quantizer = self.layer4_1_conv1_post_act_fake_quantizer(layer4_1_conv1);  layer4_1_conv1 = None\n",
      "    layer4_1_conv2 = getattr(self.layer4, \"1\").conv2(layer4_1_conv1_post_act_fake_quantizer);  layer4_1_conv1_post_act_fake_quantizer = None\n",
      "    layer4_1_conv2_post_act_fake_quantizer = self.layer4_1_conv2_post_act_fake_quantizer(layer4_1_conv2);  layer4_1_conv2 = None\n",
      "    add_7 = layer4_1_conv2_post_act_fake_quantizer + layer4_0_relu_1_post_act_fake_quantizer;  layer4_1_conv2_post_act_fake_quantizer = layer4_0_relu_1_post_act_fake_quantizer = None\n",
      "    layer4_1_relu_1 = getattr(self.layer4, \"1\").relu(add_7);  add_7 = None\n",
      "    layer4_1_relu_1_post_act_fake_quantizer = self.layer4_1_relu_1_post_act_fake_quantizer(layer4_1_relu_1);  layer4_1_relu_1 = None\n",
      "    avgpool = self.avgpool(layer4_1_relu_1_post_act_fake_quantizer);  layer4_1_relu_1_post_act_fake_quantizer = None\n",
      "    flatten = torch.flatten(avgpool, 1);  avgpool = None\n",
      "    flatten_post_act_fake_quantizer = self.flatten_post_act_fake_quantizer(flatten);  flatten = None\n",
      "    fc = self.fc(flatten_post_act_fake_quantizer);  flatten_post_act_fake_quantizer = None\n",
      "    return fc\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                                           target                                                      args                                                                                     kwargs\n",
      "-------------  ---------------------------------------------  ----------------------------------------------------------  ---------------------------------------------------------------------------------------  --------\n",
      "placeholder    x                                              x                                                           ()                                                                                       {}\n",
      "call_module    x_post_act_fake_quantizer                      x_post_act_fake_quantizer                                   (x,)                                                                                     {}\n",
      "call_module    conv1                                          conv1                                                       (x_post_act_fake_quantizer,)                                                             {}\n",
      "call_module    conv1_post_act_fake_quantizer                  conv1_post_act_fake_quantizer                               (conv1,)                                                                                 {}\n",
      "call_module    maxpool                                        maxpool                                                     (conv1_post_act_fake_quantizer,)                                                         {}\n",
      "call_module    maxpool_post_act_fake_quantizer                maxpool_post_act_fake_quantizer                             (maxpool,)                                                                               {}\n",
      "call_module    layer1_0_conv1                                 layer1.0.conv1                                              (maxpool_post_act_fake_quantizer,)                                                       {}\n",
      "call_module    layer1_0_conv1_post_act_fake_quantizer         layer1_0_conv1_post_act_fake_quantizer                      (layer1_0_conv1,)                                                                        {}\n",
      "call_module    layer1_0_conv2                                 layer1.0.conv2                                              (layer1_0_conv1_post_act_fake_quantizer,)                                                {}\n",
      "call_module    layer1_0_conv2_post_act_fake_quantizer         layer1_0_conv2_post_act_fake_quantizer                      (layer1_0_conv2,)                                                                        {}\n",
      "call_function  add                                            <built-in function add>                                     (layer1_0_conv2_post_act_fake_quantizer, maxpool_post_act_fake_quantizer)                {}\n",
      "call_module    layer1_0_relu_1                                layer1.0.relu                                               (add,)                                                                                   {}\n",
      "call_module    layer1_0_relu_1_post_act_fake_quantizer        layer1_0_relu_1_post_act_fake_quantizer                     (layer1_0_relu_1,)                                                                       {}\n",
      "call_module    layer1_1_conv1                                 layer1.1.conv1                                              (layer1_0_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer1_1_conv1_post_act_fake_quantizer         layer1_1_conv1_post_act_fake_quantizer                      (layer1_1_conv1,)                                                                        {}\n",
      "call_module    layer1_1_conv2                                 layer1.1.conv2                                              (layer1_1_conv1_post_act_fake_quantizer,)                                                {}\n",
      "call_module    layer1_1_conv2_post_act_fake_quantizer         layer1_1_conv2_post_act_fake_quantizer                      (layer1_1_conv2,)                                                                        {}\n",
      "call_function  add_1                                          <built-in function add>                                     (layer1_1_conv2_post_act_fake_quantizer, layer1_0_relu_1_post_act_fake_quantizer)        {}\n",
      "call_module    layer1_1_relu_1                                layer1.1.relu                                               (add_1,)                                                                                 {}\n",
      "call_module    layer1_1_relu_1_post_act_fake_quantizer        layer1_1_relu_1_post_act_fake_quantizer                     (layer1_1_relu_1,)                                                                       {}\n",
      "call_module    layer2_0_conv1                                 layer2.0.conv1                                              (layer1_1_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer2_0_conv1_post_act_fake_quantizer         layer2_0_conv1_post_act_fake_quantizer                      (layer2_0_conv1,)                                                                        {}\n",
      "call_module    layer2_0_conv2                                 layer2.0.conv2                                              (layer2_0_conv1_post_act_fake_quantizer,)                                                {}\n",
      "call_module    layer2_0_conv2_post_act_fake_quantizer         layer2_0_conv2_post_act_fake_quantizer                      (layer2_0_conv2,)                                                                        {}\n",
      "call_module    layer2_0_downsample_0                          layer2.0.downsample.0                                       (layer1_1_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer2_0_downsample_0_post_act_fake_quantizer  layer2_0_downsample_0_post_act_fake_quantizer               (layer2_0_downsample_0,)                                                                 {}\n",
      "call_function  add_2                                          <built-in function add>                                     (layer2_0_conv2_post_act_fake_quantizer, layer2_0_downsample_0_post_act_fake_quantizer)  {}\n",
      "call_module    layer2_0_relu_1                                layer2.0.relu                                               (add_2,)                                                                                 {}\n",
      "call_module    layer2_0_relu_1_post_act_fake_quantizer        layer2_0_relu_1_post_act_fake_quantizer                     (layer2_0_relu_1,)                                                                       {}\n",
      "call_module    layer2_1_conv1                                 layer2.1.conv1                                              (layer2_0_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer2_1_conv1_post_act_fake_quantizer         layer2_1_conv1_post_act_fake_quantizer                      (layer2_1_conv1,)                                                                        {}\n",
      "call_module    layer2_1_conv2                                 layer2.1.conv2                                              (layer2_1_conv1_post_act_fake_quantizer,)                                                {}\n",
      "call_module    layer2_1_conv2_post_act_fake_quantizer         layer2_1_conv2_post_act_fake_quantizer                      (layer2_1_conv2,)                                                                        {}\n",
      "call_function  add_3                                          <built-in function add>                                     (layer2_1_conv2_post_act_fake_quantizer, layer2_0_relu_1_post_act_fake_quantizer)        {}\n",
      "call_module    layer2_1_relu_1                                layer2.1.relu                                               (add_3,)                                                                                 {}\n",
      "call_module    layer2_1_relu_1_post_act_fake_quantizer        layer2_1_relu_1_post_act_fake_quantizer                     (layer2_1_relu_1,)                                                                       {}\n",
      "call_module    layer3_0_conv1                                 layer3.0.conv1                                              (layer2_1_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer3_0_conv1_post_act_fake_quantizer         layer3_0_conv1_post_act_fake_quantizer                      (layer3_0_conv1,)                                                                        {}\n",
      "call_module    layer3_0_conv2                                 layer3.0.conv2                                              (layer3_0_conv1_post_act_fake_quantizer,)                                                {}\n",
      "call_module    layer3_0_conv2_post_act_fake_quantizer         layer3_0_conv2_post_act_fake_quantizer                      (layer3_0_conv2,)                                                                        {}\n",
      "call_module    layer3_0_downsample_0                          layer3.0.downsample.0                                       (layer2_1_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer3_0_downsample_0_post_act_fake_quantizer  layer3_0_downsample_0_post_act_fake_quantizer               (layer3_0_downsample_0,)                                                                 {}\n",
      "call_function  add_4                                          <built-in function add>                                     (layer3_0_conv2_post_act_fake_quantizer, layer3_0_downsample_0_post_act_fake_quantizer)  {}\n",
      "call_module    layer3_0_relu_1                                layer3.0.relu                                               (add_4,)                                                                                 {}\n",
      "call_module    layer3_0_relu_1_post_act_fake_quantizer        layer3_0_relu_1_post_act_fake_quantizer                     (layer3_0_relu_1,)                                                                       {}\n",
      "call_module    layer3_1_conv1                                 layer3.1.conv1                                              (layer3_0_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer3_1_conv1_post_act_fake_quantizer         layer3_1_conv1_post_act_fake_quantizer                      (layer3_1_conv1,)                                                                        {}\n",
      "call_module    layer3_1_conv2                                 layer3.1.conv2                                              (layer3_1_conv1_post_act_fake_quantizer,)                                                {}\n",
      "call_module    layer3_1_conv2_post_act_fake_quantizer         layer3_1_conv2_post_act_fake_quantizer                      (layer3_1_conv2,)                                                                        {}\n",
      "call_function  add_5                                          <built-in function add>                                     (layer3_1_conv2_post_act_fake_quantizer, layer3_0_relu_1_post_act_fake_quantizer)        {}\n",
      "call_module    layer3_1_relu_1                                layer3.1.relu                                               (add_5,)                                                                                 {}\n",
      "call_module    layer3_1_relu_1_post_act_fake_quantizer        layer3_1_relu_1_post_act_fake_quantizer                     (layer3_1_relu_1,)                                                                       {}\n",
      "call_module    layer4_0_conv1                                 layer4.0.conv1                                              (layer3_1_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer4_0_conv1_post_act_fake_quantizer         layer4_0_conv1_post_act_fake_quantizer                      (layer4_0_conv1,)                                                                        {}\n",
      "call_module    layer4_0_conv2                                 layer4.0.conv2                                              (layer4_0_conv1_post_act_fake_quantizer,)                                                {}\n",
      "call_module    layer4_0_conv2_post_act_fake_quantizer         layer4_0_conv2_post_act_fake_quantizer                      (layer4_0_conv2,)                                                                        {}\n",
      "call_module    layer4_0_downsample_0                          layer4.0.downsample.0                                       (layer3_1_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer4_0_downsample_0_post_act_fake_quantizer  layer4_0_downsample_0_post_act_fake_quantizer               (layer4_0_downsample_0,)                                                                 {}\n",
      "call_function  add_6                                          <built-in function add>                                     (layer4_0_conv2_post_act_fake_quantizer, layer4_0_downsample_0_post_act_fake_quantizer)  {}\n",
      "call_module    layer4_0_relu_1                                layer4.0.relu                                               (add_6,)                                                                                 {}\n",
      "call_module    layer4_0_relu_1_post_act_fake_quantizer        layer4_0_relu_1_post_act_fake_quantizer                     (layer4_0_relu_1,)                                                                       {}\n",
      "call_module    layer4_1_conv1                                 layer4.1.conv1                                              (layer4_0_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_module    layer4_1_conv1_post_act_fake_quantizer         layer4_1_conv1_post_act_fake_quantizer                      (layer4_1_conv1,)                                                                        {}\n",
      "call_module    layer4_1_conv2                                 layer4.1.conv2                                              (layer4_1_conv1_post_act_fake_quantizer,)                                                {}\n",
      "call_module    layer4_1_conv2_post_act_fake_quantizer         layer4_1_conv2_post_act_fake_quantizer                      (layer4_1_conv2,)                                                                        {}\n",
      "call_function  add_7                                          <built-in function add>                                     (layer4_1_conv2_post_act_fake_quantizer, layer4_0_relu_1_post_act_fake_quantizer)        {}\n",
      "call_module    layer4_1_relu_1                                layer4.1.relu                                               (add_7,)                                                                                 {}\n",
      "call_module    layer4_1_relu_1_post_act_fake_quantizer        layer4_1_relu_1_post_act_fake_quantizer                     (layer4_1_relu_1,)                                                                       {}\n",
      "call_module    avgpool                                        avgpool                                                     (layer4_1_relu_1_post_act_fake_quantizer,)                                               {}\n",
      "call_function  flatten                                        <built-in method flatten of type object at 0x7f5e3b6841a0>  (avgpool, 1)                                                                             {}\n",
      "call_module    flatten_post_act_fake_quantizer                flatten_post_act_fake_quantizer                             (flatten,)                                                                               {}\n",
      "call_module    fc                                             fc                                                          (flatten_post_act_fake_quantizer,)                                                       {}\n",
      "output         output                                         output                                                      (fc,)                                                                                    {}\n"
     ]
    }
   ],
   "source": [
    "model_prepared.graph.print_tabular()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a3057a8c36782cd4f6331a3ae51fc068edd966d636a8a359d99ae7e41376aa0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
